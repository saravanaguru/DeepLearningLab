{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f5ec92-1f6b-4513-a032-a41400f965f6",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objective\n",
    "\n",
    "Understand how **backpropagation** adjusts neural network weights using error gradients â€” in a simple, human-like context (a robot learning to cook).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ³ Real-Life Analogy: The Robot Chef\n",
    "\n",
    "Imagine you built a **Robot Chef ðŸ¤– named CookBot**.  \n",
    "CookBot is learning to make a **perfect dosa**.\n",
    "\n",
    "It takes **two inputs**:\n",
    "\n",
    "- ðŸ”¥ Heat level (xâ‚)  \n",
    "- ðŸ§‚ Salt level (xâ‚‚)  \n",
    "\n",
    "and produces **one output**:\n",
    "\n",
    "- ðŸ¥ž Crispiness level (Å·)\n",
    "\n",
    "You, as the teacher, know the *desired crispiness* (y), and you want CookBot to *learn* the perfect combination of heat and salt.\n",
    "\n",
    "The robot starts with random guesses for:\n",
    "\n",
    "- Weight for heat (**wâ‚**)  \n",
    "- Weight for salt (**wâ‚‚**)  \n",
    "- Bias (**b**)  \n",
    "\n",
    "Then it adjusts these using **backpropagation** â€” just like how a chef tweaks the recipe after tasting each trial batch.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Neural Network Setup\n",
    "\n",
    "| **Layer**        | **Details**                          |\n",
    "|------------------:|--------------------------------------|\n",
    "| **Input**         | xâ‚ = Heat, xâ‚‚ = Salt                 |\n",
    "| **Hidden Layer**  | 2 neurons with sigmoid activation    |\n",
    "| **Output Layer**  | 1 neuron (crispiness)                |\n",
    "| **Loss Function** | Mean Squared Error (MSE)             |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Step-by-Step Explanation\n",
    "\n",
    "### 1ï¸âƒ£ Forward Pass\n",
    "CookBot predicts crispiness using current weights and activations.\n",
    "\n",
    "### 2ï¸âƒ£ Compute Error\n",
    "It tastes the dosa (compares prediction Å· with true y).\n",
    "\n",
    "### 3ï¸âƒ£ Backpropagate\n",
    "CookBot computes *how much each ingredient contributed to the mistake* (gradients).\n",
    "\n",
    "### 4ï¸âƒ£ Update Weights\n",
    "It tweaks the recipe (weights) slightly using the learning rate (Î·).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b302958-b657-477e-8a5e-19f2ceb140ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss: 0.2880\n",
      "Epoch 2000  Loss: 0.0034\n",
      "Epoch 4000  Loss: 0.0010\n",
      "Epoch 6000  Loss: 0.0006\n",
      "Epoch 8000  Loss: 0.0004\n",
      "\n",
      "Final predicted crispiness levels:\n",
      "[[0.01920538]\n",
      " [0.98342777]\n",
      " [0.98341196]\n",
      " [0.01716291]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Input data: Heat and Salt\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])   # possible combinations\n",
    "\n",
    "# Desired crispiness output (teacher feedback)\n",
    "y = np.array([[0], [1], [1], [0]])  # we want balanced crispy dosa!\n",
    "\n",
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "w1 = np.random.rand(2, 2)\n",
    "w2 = np.random.rand(2, 1)\n",
    "b1 = np.random.rand(1, 2)\n",
    "b2 = np.random.rand(1, 1)\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10000):\n",
    "    # ---- Forward pass ----\n",
    "    hidden_input = np.dot(X, w1) + b1\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    final_input = np.dot(hidden_output, w2) + b2\n",
    "    final_output = sigmoid(final_input)\n",
    "\n",
    "    # ---- Compute error ----\n",
    "    error = y - final_output\n",
    "\n",
    "    # ---- Backpropagation ----\n",
    "    d_output = error * sigmoid_derivative(final_output)\n",
    "    error_hidden = d_output.dot(w2.T)\n",
    "    d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # ---- Update weights ----\n",
    "    w2 += hidden_output.T.dot(d_output) * learning_rate\n",
    "    b2 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    w1 += X.T.dot(d_hidden) * learning_rate\n",
    "    b1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if epoch % 2000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}  Loss: {loss:.4f}\")\n",
    "\n",
    "# Final prediction\n",
    "print(\"\\nFinal predicted crispiness levels:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc40fe-3ec8-4b0a-b41d-5f092f939e5d",
   "metadata": {},
   "source": [
    "## ðŸ§  Student Exercises on Backpropagation (BPN)\n",
    "\n",
    "These exercises are designed to help you connect the **mathematics of error correction** with **real-world learning** â€” the way humans (and machines) refine their actions from feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© **Exercise 1: The Coffee Machine Learner**\n",
    "\n",
    "A smart coffee machine is learning to make coffee at the *perfect temperature (output)* based on two inputs:\n",
    "- â˜• Coffee powder amount (xâ‚)\n",
    "- ðŸ”¥ Water temperature (xâ‚‚)\n",
    "\n",
    "It has:\n",
    "- 2 input neurons (xâ‚, xâ‚‚)\n",
    "- 2 hidden neurons (with sigmoid activation)\n",
    "- 1 output neuron (predicted taste score Å·)\n",
    "\n",
    "**Tasks:**\n",
    "1. Randomly initialize small weights and biases.\n",
    "2. For a given training example (xâ‚=0.6, xâ‚‚=0.9, y=1.0),  \n",
    "   perform **one full forward and backward pass**.\n",
    "3. Show how the **weight updates** (Î”w) depend on the error.\n",
    "4. Plot the loss after each epoch for 50 iterations.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ€ **Exercise 2: Weather Prediction Using BPN**\n",
    "\n",
    "You are building a small neural network to predict **whether it will rain tomorrow** (yes/no) based on:\n",
    "- ðŸŒ¡ï¸ Temperature (xâ‚)\n",
    "- ðŸ’§ Humidity (xâ‚‚)\n",
    "- ðŸŒ¬ï¸ Wind speed (xâ‚ƒ)\n",
    "\n",
    "**Tasks:**\n",
    "1. Define a 3â€“2â€“1 neural network.\n",
    "2. Implement the **sigmoid activation** and **Mean Squared Error (MSE)**.\n",
    "3. Generate synthetic weather data (10 samples).\n",
    "4. Train using **batch gradient descent**.\n",
    "5. Visualize how the **loss decreases over epochs**.\n",
    "\n",
    "**Hint:** Think of the model as â€œlearning the climate intuitionâ€ just like a farmer who adjusts his guess each day!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¤– **Exercise 3: Emotion Detector **\n",
    "\n",
    "You are training a simple emotion detector that classifies:\n",
    "- Input: voice pitch (xâ‚), speaking speed (xâ‚‚)\n",
    "- Output: emotion intensity (Å·)\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a small dataset of 8â€“10 examples with different xâ‚, xâ‚‚, and target y.  \n",
    "2. Build a simple **2â€“3â€“1 neural network** in NumPy.  \n",
    "3. Implement **forward pass**, **error calculation**, and **backpropagation** manually.  \n",
    "4. Show how weight updates happen for one training cycle.  \n",
    "5. Reflect: How does BPN mimic human emotional correction during communication?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš— **Exercise 4: Self-Learning Braking System**\n",
    "\n",
    "An autonomous car learns when to apply brakes:\n",
    "- Inputs: distance from object (xâ‚), current speed (xâ‚‚)\n",
    "- Output: brake force (Å·)\n",
    "\n",
    "**Tasks:**\n",
    "1. Initialize weights randomly and simulate a few examples.  \n",
    "2. Use **gradient descent** to minimize the prediction error.  \n",
    "3. Plot:\n",
    "   - Loss curve  \n",
    "   - Output vs. target comparison  \n",
    "4. Discuss how **backpropagation helps prevent accidents** through error feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒ¾ **Exercise 5: Smart Irrigation Controller**\n",
    "\n",
    "The controller adjusts watering level (output) based on:\n",
    "- Soil moisture (xâ‚)\n",
    "- Temperature (xâ‚‚)\n",
    "- Sunlight intensity (xâ‚ƒ)\n",
    "\n",
    "**Tasks:**\n",
    "1. Simulate 10 data points.  \n",
    "2. Implement a **3â€“3â€“1 network** with ReLU hidden activation and sigmoid output.  \n",
    "3. Use backpropagation to train for 100 epochs.  \n",
    "4. Interpret: What happens when the learning rate is too high or too low?\n",
    "\n",
    "---\n",
    "\n",
    "### âœï¸ **Reflective Question**\n",
    "\n",
    "> In all these examples, the **core idea of backpropagation** is â€œlearning from mistakes.â€  \n",
    "> How does this computationally reflect **human trial-and-error learning** in daily life?\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
